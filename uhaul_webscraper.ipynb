{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "1cc7b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import pandas as pd\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from geopy.geocoders import Nominatim\n",
    "import requests\n",
    "from datetime import date\n",
    "import time\n",
    "from csv import writer\n",
    "\n",
    "try:\n",
    "    # Close any existing WebDrivers\n",
    "    driver.close() \n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "#setting up driver\n",
    "s=Service('C:\\\\Users\\Korisnik\\PycharmProjects\\soupokusaji\\chromedriver.exe')\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "#defining waittime for driver\n",
    "waittime = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "914a2195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that goes through displayed results and scrapes data about facilities and available units\n",
    "def facilityscrape ():\n",
    "    # finding facility (entity) codes(numbers) that will be later used to create a unique xpath for \"view rates\" button\n",
    "    entities = driver.find_elements(By.NAME, 'EntityNumber')\n",
    "    \n",
    "    # saving these numbers in a list to loop through facilities\n",
    "    facilities = []\n",
    "    for entity in entities[1::2]:\n",
    "        facilities.append(entity.get_attribute('value'))\n",
    "\n",
    "    # check if facility is already scraped, if not then add its entity number to list of unique facilities\n",
    "    for facility in facilities:\n",
    "        if facility not in unique_facilities:\n",
    "            unique_facilities.append(facility)\n",
    "            WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"viewRates_' + facility + '\"]'))).click()\n",
    "            \n",
    "            # scraping facility's url\n",
    "            get_url = driver.current_url\n",
    "            \n",
    "            # accessing html\n",
    "            html_text = requests.get(get_url).text\n",
    "            soup = BeautifulSoup(html_text, 'lxml')\n",
    "\n",
    "            # scraping facility's address\n",
    "            \n",
    "            # addresses may contain additional instructions inside parentheses, they are unnecessary so they're disposed of\n",
    "            address_ = soup.find('address').text\n",
    "            if \"(\" in address_:\n",
    "                address_, x1 = address_.split(\"(\")\n",
    "            \n",
    "            # adresses contain weird blank spaces that are easiest to get rid of by splitting the string and joining it again\n",
    "            splitt= address_.split()\n",
    "            address_= ' '.join(splitt)\n",
    "            \n",
    "            # splitting address into street, state and zipcode\n",
    "            occur = address_.count(',')\n",
    "            if occur == 1:\n",
    "                address_, state = address_.split(\",\")\n",
    "            elif occur == 2:\n",
    "                address_, region, state = address_.split(\",\")\n",
    "                address_ = address_ + region\n",
    "            elif occur == 0:\n",
    "                state = \"- -\"\n",
    "            else:\n",
    "                add1, add2, region, state = address_.split(\",\")\n",
    "                address_ = add1 + add2 + region\n",
    "                \n",
    "\n",
    "            state, zipp = state.split()           \n",
    "            \n",
    "\n",
    "            # scraping facility's name\n",
    "            \n",
    "            # if facility is u-haul's affiliate, its name is formatted differently from other facilities,\n",
    "            # so the code first checks if there exists name formatted like that\n",
    "            if driver.find_elements(By.XPATH, '//*[@id=\"mainRow\"]/div/div/div/div/div/div[1]/h1'):\n",
    "                \n",
    "                name_ = soup.find('h1', class_=\"collapse\").text\n",
    "                name_ = name_.strip()\n",
    "                print(name_)\n",
    "\n",
    "            else:\n",
    "                name_ = soup.find('h2', class_=\"collapse-half text-dull text-xl text-semibold\").text\n",
    "                name_ = name_.strip()\n",
    "            \n",
    "\n",
    "            # getting todays date\n",
    "            today = date.today()\n",
    "            thedate = today.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            #facility features will contain date, facility id, name, address and url\n",
    "            fac_features = []\n",
    "            \n",
    "            value0 = thedate\n",
    "            value1 = facility\n",
    "            value2 = name_\n",
    "            value3 = address_\n",
    "            value4 = state\n",
    "            value5 = zipp\n",
    "            value6 = get_url\n",
    "            \n",
    "            fac_features.append(value0)\n",
    "            fac_features.append(value1)\n",
    "            fac_features.append(value2)\n",
    "            fac_features.append(value3)\n",
    "            fac_features.append(value4)\n",
    "            fac_features.append(value5)\n",
    "            fac_features.append(value6)\n",
    "\n",
    "\n",
    "            facility_masterlist.append(fac_features)\n",
    "\n",
    "\n",
    "            # finding all displayed units\n",
    "            units = soup.find_all('div', class_=\"grid-x grid-margin-x align-left medium-grid-expand-x large-align-middle\")\n",
    "            \n",
    "            # looping through units\n",
    "            for unit in units:\n",
    "                \n",
    "                # scraping unit description\n",
    "                unit_desc = unit.p.text\n",
    "                \n",
    "                # excluding units that aren't the regular self storage units - RV/Boat, lockers, wine, office and warehouse\n",
    "                if \"RV/Boat\" not in unit_desc and \"Lockers\" not in unit_desc and \"Office\" not in unit_desc and \"Wine\" not in unit_desc and \"Warehouse\" not in unit_desc:\n",
    "                    \n",
    "                    # scraping unit size and splitting it into description (small, medium, large) and\n",
    "                    # actual size in numbers \n",
    "                    unit_size = unit.h4.text.strip()\n",
    "                    size_desc, size = unit_size.split(\" | \")\n",
    "                    \n",
    "                    # scraping unit price\n",
    "                    unit_price = unit.find('b', class_=\"text-lg\").text.strip()\n",
    "                    unit_price =unit_price.strip(\"$\")\n",
    "                    \n",
    "                    # formatting unit description to get rid of blank spaces\n",
    "                    splitted= unit_desc.split()\n",
    "                    unit_desc= ' '.join(splitted)\n",
    "\n",
    "\n",
    "                    # saving scraped data one by one to storage_features list, then appending that list\n",
    "                    # as one row to units_masterlist\n",
    "                    storage_features = []\n",
    "\n",
    "                    value_1 = thedate\n",
    "                    value_2 = facility\n",
    "                    value_3 = unit_idd[0]\n",
    "                    value_4 = size_desc\n",
    "                    value_5 = size\n",
    "                    value_6 = unit_desc\n",
    "                    value_7 = unit_price\n",
    "\n",
    "                    storage_features.append(value_1)\n",
    "                    storage_features.append(value_2)\n",
    "                    storage_features.append(value_3)\n",
    "                    storage_features.append(value_4)\n",
    "                    storage_features.append(value_5)\n",
    "                    storage_features.append(value_6)\n",
    "                    storage_features.append(value_7)\n",
    "                    \n",
    "                    units_masterlist.append(storage_features)\n",
    "\n",
    "\n",
    "                    # increasing number for unit id\n",
    "                    unit_idd[0]+= 1\n",
    "                        \n",
    "\n",
    "            #goes back to results page to scrape the next facility from results\n",
    "            driver.back()\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "1e108ce7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# function that goes through 4 pages of results\n",
    "def resultsloop ():   \n",
    "            # invoking function that scrapes the results\n",
    "            facilityscrape()\n",
    "            # making a pause in order not to overwhelm the website's server\n",
    "            time.sleep(1.5)\n",
    "            \n",
    "            # moving to the next 6 results\n",
    "            # for some cities there is less than 4 pages, so the function checks if there is \"more locations\" button\n",
    "            if driver.find_elements(By.XPATH, '//*[@id=\"locationsResults\"]/ul[2]/li/a[2]'):\n",
    "                \n",
    "                WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"locationsResults\"]/ul[2]/li/a[2]'))).click()\n",
    "                # invoking function that scrapes the results \n",
    "                facilityscrape()\n",
    "                time.sleep(1.5)\n",
    "                \n",
    "                # checking if there is \"more locations\" button before moving onto next 6 results\n",
    "                if driver.find_elements(By.XPATH, '//*[@id=\"locationsResults\"]/ul[2]/li[2]/a[2]'):\n",
    "\n",
    "                    WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"locationsResults\"]/ul[2]/li[2]/a[2]'))).click()\n",
    "                    # invoking function that scrapes the results \n",
    "                    facilityscrape()\n",
    "                    time.sleep(1.5)\n",
    "\n",
    "                    # checking if there is \"more locations\" button before moving onto last 2 results\n",
    "                    if driver.find_elements(By.XPATH, '//*[@id=\"locationsResults\"]/ul[2]/li[2]/a[2]'):\n",
    "                    \n",
    "                        WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"locationsResults\"]/ul[2]/li[2]/a[2]'))).click()\n",
    "                        facilityscrape()\n",
    "                        time.sleep(1.5)\n",
    "\n",
    "            \n",
    "            \n",
    "# function that goes through the list of cities, searches for the selected city and calls function for scraping            \n",
    "def cityloop(city):\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # checks if there is a search form on the page, in some circumstances due to previous city malfunction there \n",
    "        # won't be a search form, so this covers that case to prevent code from looping through cities without\n",
    "        # scraping any data\n",
    "        if driver.find_elements(By.XPATH, '//*[@id=\"locationSearchForm\"]/div[2]/div[2]/button'):\n",
    "            WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"movingFromInput\"]'))).clear()\n",
    "            # insert text in search bar\n",
    "            WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"movingFromInput\"]'))).send_keys(city)\n",
    "            # click submit to search for city \n",
    "            WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"locationSearchForm\"]/div[2]/div[2]/button'))).click()\n",
    "            \n",
    "            # in case there isn't an exact match to city+state code, it will move on to the next city\n",
    "            if driver.find_elements(By.XPATH, '//*[@id=\"locationAmbigAddresses\"]/p') or driver.find_elements(By.XPATH, '//*[@id=\"locationSearchForm\"]/fieldset/div/div[1]/ul/li'):\n",
    "                print(\"would've thrown an error but im nice \" + city)\n",
    "                url='https://www.uhaul.com/Storage/WY/Results/'\n",
    "                driver.get(url)\n",
    "            else:\n",
    "                # if the search is successful, this invokes function that scrapes the results\n",
    "                resultsloop()\n",
    "                print(\"finished scraping \" + city)\n",
    "        \n",
    "        #in case of malfunction, sets up the page with valid search form for next city \n",
    "        else:\n",
    "            print(\"something is wrong with (probably couldn't scrape city before): \" + city)\n",
    "            url='https://www.uhaul.com/Storage/WY/Results/'\n",
    "            driver.get(url)\n",
    "            \n",
    "            \n",
    "\n",
    "         \n",
    "            \n",
    "    # in case of exception\n",
    "    except Exception:\n",
    "        print(\"didnt fully scrape \" + city)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "1d4b2fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list that will hold all facility data, and will be used to create a csv file\n",
    "facility_masterlist = []\n",
    "# creating a list that will have all scraped units, and will be used to create a csv file\n",
    "units_masterlist=[]\n",
    "# creating a set of entity numbers so that duplicates aren't scraped\n",
    "unique_facilities = []\n",
    "# setting up internal unit id\n",
    "unit_idd=[0]\n",
    "# opening uhaul storage domain, specifically results for wyoming since it's easier\n",
    "# to search for cities from the results page of some other city/state\n",
    "url='https://www.uhaul.com/Storage/WY/Results/'\n",
    "driver.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482fb798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening an excel file containing a list of all major cities in the US, list is taken from Britannica\n",
    "df = pd.read_excel('all_states.xlsx')    \n",
    "\n",
    "# taking city + state code and feeding it to the function that will loop through the cities and scrape results\n",
    "result = df['city'] + \" \" + df['state_code']\n",
    "result.apply(cityloop)\n",
    "\n",
    "#saving facility master list as csv file with unique name containing today's date\n",
    "df2 = pd.DataFrame(facility_masterlist, columns=['date', 'facility_id', 'name', 'address', 'region', 'postal_code', 'url'])\n",
    "today = date.today()\n",
    "thedate = today.strftime(\"%d-%m-%Y\")\n",
    "namee = 'facilities-' + thedate + \".csv\"\n",
    "df2.to_csv(namee, index=False, sep=',')\n",
    "print('Created facility skrejp')\n",
    "\n",
    "#saving units master list as csv file with unique name containing today's date\n",
    "df3 = pd.DataFrame(units_masterlist, columns=['date', 'location_external_id', 'internal_id', 'size_description', 'size', 'description', 'price'])\n",
    "namee_ = 'units-' + thedate + \".csv\"\n",
    "df3.to_csv(namee_, index=False, sep=',')\n",
    "print('Created new master list file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef3d123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766f4a92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
